\documentclass[12pt]{article}
\begin{document}
Stochastic modeling is an important part of the field of statistics, as data produced by chance or seemingly random variables are not effectively analyzed by conventional models. These models are highly effective at modeling many natural patterns such as weather prediction from observed data, so methods for performing these approximations are well-researched, but in comparison, the limiting factors and reliability of these models is given less attention, largely due to lesser applicability in solving real-world problems. The purpose of this study was to analyze the relationship between the complexity of stochastic systems and how accurately they can be reverse-engineered from the data it produced, provided the transition rules between states are both readily available and completely deterministic, to observe the influence of variables like memory and variance given a probabilistic data input and a deterministic reverse-engineering algorithm. The stochastic models being analyzed were designed to mimic systems of weighted dice, where each system is defined to have a defined number of outputs and states (dice) alongside a defined set of transition rules between each of the die. 1000 of these die systems were generated with random weights and transition rules, then each was reverse-engineered using an algorithm that uses histogram-based approximation to calculate the matching weights that would exist for each die within the system. Once this calculation was completed, these weights and transition rules are used to calculate an error score between the input (true) model and the output (approximated) model. For each input model, a complexity score was then calculated based on its weights and transition rules, and then the trend between complexity and error for each model was compared, showing that increases to memory result in a considerably larger growth in the maximum complexity of the system with a relatively smaller change in maximum error.
\end{document}